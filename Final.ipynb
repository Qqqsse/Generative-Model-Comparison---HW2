{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc/xMmoek1Qr7nJ3R5gYgK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qqqsse/Generative-Model-Comparison---HW2/blob/main/Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW4jD890fmll"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "完整生成模型訓練程式碼\n",
        "包含: VAE, GAN, cGAN, Diffusion Model\n",
        "MNIST 手寫數字生成\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 1. VAE 函式庫及定義 ====================\n",
        "print(\"=\" * 60)\n",
        "print(\"載入 VAE 模組...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# 設定隨機種子\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}\\n')\n",
        "\n",
        "# VAE 模型定義\n",
        "class ImprovedVAE(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
        "        super(ImprovedVAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
        "        self.fc_mu = nn.Linear(hidden_dim // 2, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim // 2, latent_dim)\n",
        "\n",
        "        self.fc3 = nn.Linear(latent_dim, hidden_dim // 2)\n",
        "        self.fc4 = nn.Linear(hidden_dim // 2, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        h1 = self.dropout(h1)\n",
        "        h2 = F.relu(self.fc2(h1))\n",
        "        mu = self.fc_mu(h2)\n",
        "        logvar = self.fc_logvar(h2)\n",
        "        logvar = torch.clamp(logvar, -10, 10)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        h3 = self.dropout(h3)\n",
        "        h4 = F.relu(self.fc4(h3))\n",
        "        return torch.sigmoid(self.fc5(h4))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon = self.decode(z)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "# VAE 損失函數\n",
        "def improved_vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
        "    batch_size = x.size(0)\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum') / batch_size\n",
        "    KLD_per_dim = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    free_bits = 0.5\n",
        "    KLD_per_dim = torch.clamp(KLD_per_dim, min=free_bits)\n",
        "    KLD = torch.sum(KLD_per_dim) / batch_size\n",
        "    return BCE + beta * KLD, BCE, KLD\n",
        "\n",
        "# VAE 訓練函數\n",
        "def train_vae(model, train_loader, optimizer, epoch, beta=1.0):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_bce = 0\n",
        "    train_kld = 0\n",
        "    pbar = tqdm(train_loader, desc=f'VAE Epoch {epoch}')\n",
        "\n",
        "    for batch_idx, (data, _) in enumerate(pbar):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss, bce, kld = improved_vae_loss(recon_batch, data, mu, logvar, beta)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "        train_bce += bce.item() * data.size(0)\n",
        "        train_kld += kld.item() * data.size(0)\n",
        "        optimizer.step()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'bce': f'{bce.item():.4f}', 'kld': f'{kld.item():.4f}'})\n",
        "\n",
        "    n = len(train_loader.dataset)\n",
        "    return train_loss / n, train_bce / n, train_kld / n\n",
        "\n",
        "def test_vae(model, test_loader, beta=1.0):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_bce = 0\n",
        "    test_kld = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss, bce, kld = improved_vae_loss(recon_batch, data, mu, logvar, beta)\n",
        "            test_loss += loss.item() * data.size(0)\n",
        "            test_bce += bce.item() * data.size(0)\n",
        "            test_kld += kld.item() * data.size(0)\n",
        "    n = len(test_loader.dataset)\n",
        "    return test_loss / n, test_bce / n, test_kld / n\n",
        "\n",
        "# VAE 視覺化函數\n",
        "def generate_vae_images(model, num_images=10, latent_dim=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_images, latent_dim).to(device)\n",
        "        samples = model.decode(z)\n",
        "        samples = samples.view(num_images, 1, 28, 28).cpu()\n",
        "    return samples\n",
        "\n",
        "def visualize_generated(images, title='Generated Images', save_path=None):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.imshow(images[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
        "        ax.axis('off')\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_vae_training_curves(train_losses, test_losses, train_klds, test_klds, save_path=None):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    ax1.plot(train_losses, label='Train Loss', linewidth=2)\n",
        "    ax1.plot(test_losses, label='Test Loss', linewidth=2)\n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('VAE Total Loss', fontsize=14)\n",
        "    ax1.legend(fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2.plot(train_klds, label='Train KLD', linewidth=2)\n",
        "    ax2.plot(test_klds, label='Test KLD', linewidth=2)\n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('KLD', fontsize=12)\n",
        "    ax2.set_title('KL Divergence', fontsize=14)\n",
        "    ax2.legend(fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "K25nkJf1f4di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 2. VAE 執行訓練 ====================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"開始訓練 VAE (Variational Autoencoder)\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# 載入資料\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# VAE 訓練參數\n",
        "latent_dim = 20\n",
        "hidden_dim = 400\n",
        "learning_rate = 1e-3\n",
        "num_epochs_vae = 50\n",
        "beta_start = 0.5\n",
        "beta_end = 4.0\n",
        "warmup_epochs = 20\n",
        "\n",
        "# 建立模型\n",
        "vae_model = ImprovedVAE(input_dim=784, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
        "vae_optimizer = optim.Adam(vae_model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "vae_scheduler = optim.lr_scheduler.ReduceLROnPlateau(vae_optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "print(f'VAE 配置:')\n",
        "print(f'Latent Dim: {latent_dim}, Hidden Dim: {hidden_dim}')\n",
        "print(f'Learning Rate: {learning_rate}, Epochs: {num_epochs_vae}')\n",
        "print(f'β Range: {beta_start} -> {beta_end}\\n')\n",
        "\n",
        "train_losses_vae = []\n",
        "test_losses_vae = []\n",
        "train_klds_vae = []\n",
        "test_klds_vae = []\n",
        "\n",
        "for epoch in range(1, num_epochs_vae + 1):\n",
        "    if epoch <= warmup_epochs:\n",
        "        beta = beta_start + (beta_end - beta_start) * (epoch / warmup_epochs)\n",
        "    else:\n",
        "        beta = beta_end\n",
        "\n",
        "    train_loss, train_bce, train_kld = train_vae(vae_model, train_loader, vae_optimizer, epoch, beta)\n",
        "    test_loss, test_bce, test_kld = test_vae(vae_model, test_loader, beta)\n",
        "\n",
        "    train_losses_vae.append(train_loss)\n",
        "    test_losses_vae.append(test_loss)\n",
        "    train_klds_vae.append(train_kld)\n",
        "    test_klds_vae.append(test_kld)\n",
        "\n",
        "    vae_scheduler.step(test_loss)\n",
        "    current_lr = vae_optimizer.param_groups[0]['lr']\n",
        "    print(f'Epoch {epoch}: Loss={test_loss:.4f}, BCE={test_bce:.4f}, KLD={test_kld:.4f}, β={beta:.2f}, LR={current_lr:.6f}')\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        samples = generate_vae_images(vae_model, num_images=10, latent_dim=latent_dim)\n",
        "        visualize_generated(samples, title=f'VAE Generated (Epoch {epoch})')\n",
        "\n",
        "# VAE 最終結果\n",
        "plot_vae_training_curves(train_losses_vae, test_losses_vae, train_klds_vae, test_klds_vae, 'vae_training_curve.png')\n",
        "final_vae_samples = generate_vae_images(vae_model, num_images=10, latent_dim=latent_dim)\n",
        "visualize_generated(final_vae_samples, title='VAE Final Generated Images', save_path='vae_final_results.png')\n",
        "torch.save(vae_model.state_dict(), 'vae_mnist_improved.pth')\n",
        "print(f'\\nVAE 訓練完成! 最終測試損失: {test_losses_vae[-1]:.4f}\\n')"
      ],
      "metadata": {
        "id": "sPZOswg5f8Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 3. GAN 函式庫及定義 ====================\n",
        "print(\"=\" * 60)\n",
        "print(\"載入 GAN 模組...\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# GAN 模型定義\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100, hidden_dim=256):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
        "            nn.BatchNorm1d(hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
        "            nn.BatchNorm1d(hidden_dim * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim * 4, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        img = img.view(img.size(0), 1, 28, 28)\n",
        "        return img\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, hidden_dim=256):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, hidden_dim * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        validity = self.model(img_flat)\n",
        "        return validity\n",
        "\n",
        "# GAN 訓練函數\n",
        "def train_gan(generator, discriminator, train_loader, optimizer_G, optimizer_D, epoch, device):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "    adversarial_loss = nn.BCELoss()\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'GAN Epoch {epoch}')\n",
        "    for batch_idx, (real_imgs, _) in enumerate(pbar):\n",
        "        batch_size = real_imgs.size(0)\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        valid = torch.ones(batch_size, 1).to(device)\n",
        "        fake = torch.zeros(batch_size, 1).to(device)\n",
        "        real_imgs = real_imgs * 2 - 1\n",
        "\n",
        "        # 訓練 Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
        "        z = torch.randn(batch_size, 100).to(device)\n",
        "        fake_imgs = generator(z)\n",
        "        fake_loss = adversarial_loss(discriminator(fake_imgs.detach()), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # 訓練 Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        z = torch.randn(batch_size, 100).to(device)\n",
        "        gen_imgs = generator(z)\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        g_losses.append(g_loss.item())\n",
        "        d_losses.append(d_loss.item())\n",
        "        pbar.set_postfix({'D_loss': d_loss.item(), 'G_loss': g_loss.item()})\n",
        "\n",
        "    return np.mean(g_losses), np.mean(d_losses)\n",
        "\n",
        "# GAN 生成函數\n",
        "def generate_gan_images(generator, num_images=10, latent_dim=100):\n",
        "    generator.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(num_images, latent_dim).to(device)\n",
        "        samples = generator(z)\n",
        "        samples = (samples + 1) / 2\n",
        "        samples = samples.cpu()\n",
        "    return samples\n",
        "\n",
        "def plot_gan_training_curve(g_losses, d_losses, save_path=None):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(g_losses, label='Generator Loss', alpha=0.7)\n",
        "    plt.plot(d_losses, label='Discriminator Loss', alpha=0.7)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('GAN Training Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "P4tsVGgxgBtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 4. GAN 執行訓練 ====================\n",
        "print(\"=\" * 60)\n",
        "print(\"開始訓練 GAN (Generative Adversarial Network)\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# GAN 訓練參數\n",
        "latent_dim_gan = 100\n",
        "hidden_dim_gan = 512\n",
        "G_lr = 2e-4\n",
        "D_lr = 2 * G_lr\n",
        "num_epochs_gan = 100\n",
        "\n",
        "# 建立模型\n",
        "gan_generator = Generator(latent_dim=latent_dim_gan, hidden_dim=hidden_dim_gan).to(device)\n",
        "gan_discriminator = Discriminator(hidden_dim=hidden_dim_gan).to(device)\n",
        "optimizer_G = optim.Adam(gan_generator.parameters(), lr=G_lr, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(gan_discriminator.parameters(), lr=D_lr, betas=(0.5, 0.999))\n",
        "\n",
        "print(f'GAN 配置:')\n",
        "print(f'Latent Dim: {latent_dim_gan}, Hidden Dim: {hidden_dim_gan}')\n",
        "print(f'Learning Rate: {G_lr}, Epochs: {num_epochs_gan}\\n')\n",
        "\n",
        "gan_g_losses = []\n",
        "gan_d_losses = []\n",
        "\n",
        "for epoch in range(1, num_epochs_gan + 1):\n",
        "    g_loss, d_loss = train_gan(gan_generator, gan_discriminator, train_loader, optimizer_G, optimizer_D, epoch, device)\n",
        "    gan_g_losses.append(g_loss)\n",
        "    gan_d_losses.append(d_loss)\n",
        "    print(f'Epoch {epoch}: G_loss = {g_loss:.4f}, D_loss = {d_loss:.4f}')\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        samples = generate_gan_images(gan_generator, num_images=10, latent_dim=latent_dim_gan)\n",
        "        visualize_generated(samples, title=f'GAN Generated (Epoch {epoch})')\n",
        "\n",
        "# GAN 最終結果\n",
        "plot_gan_training_curve(gan_g_losses, gan_d_losses, save_path='gan_training_curve.png')\n",
        "final_gan_samples = generate_gan_images(gan_generator, num_images=10, latent_dim=latent_dim_gan)\n",
        "visualize_generated(final_gan_samples, title='GAN Final Generated Images', save_path='gan_final_results.png')\n",
        "torch.save(gan_generator.state_dict(), 'gan_generator.pth')\n",
        "torch.save(gan_discriminator.state_dict(), 'gan_discriminator.pth')\n",
        "print(f'\\nGAN 訓練完成! 最終 G_loss: {gan_g_losses[-1]:.4f}, D_loss: {gan_d_losses[-1]:.4f}\\n')"
      ],
      "metadata": {
        "id": "64ZPbcUkgL9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 5. cGAN 函式庫及定義 ====================\n",
        "print(\"=\" * 60)\n",
        "print(\"載入 cGAN 模組...\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "# cGAN 模型定義\n",
        "class ConditionalGenerator(nn.Module):\n",
        "    def __init__(self, latent_dim=100, num_classes=10, hidden_dim=256):\n",
        "        super(ConditionalGenerator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + num_classes, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
        "            nn.BatchNorm1d(hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
        "            nn.BatchNorm1d(hidden_dim * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim * 4, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        label_input = self.label_emb(labels)\n",
        "        gen_input = torch.cat([z, label_input], dim=1)\n",
        "        img = self.model(gen_input)\n",
        "        img = img.view(img.size(0), 1, 28, 28)\n",
        "        return img\n",
        "\n",
        "class ConditionalDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes=10, hidden_dim=256):\n",
        "        super(ConditionalDiscriminator, self).__init__()\n",
        "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784 + num_classes, hidden_dim * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        img_flat = img.view(img.size(0), -1)\n",
        "        label_input = self.label_emb(labels)\n",
        "        d_input = torch.cat([img_flat, label_input], dim=1)\n",
        "        validity = self.model(d_input)\n",
        "        return validity\n",
        "\n",
        "# cGAN 訓練函數\n",
        "def train_cgan(generator, discriminator, train_loader, optimizer_G, optimizer_D, epoch, device):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "    g_losses = []\n",
        "    d_losses = []\n",
        "    adversarial_loss = nn.BCELoss()\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'cGAN Epoch {epoch}')\n",
        "    for batch_idx, (real_imgs, labels) in enumerate(pbar):\n",
        "        batch_size = real_imgs.size(0)\n",
        "        real_imgs = real_imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        valid = torch.ones(batch_size, 1).to(device) * 0.85\n",
        "        fake = torch.zeros(batch_size, 1).to(device)\n",
        "        real_imgs = real_imgs * 2 - 1\n",
        "\n",
        "        # 訓練 Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)\n",
        "        z = torch.randn(batch_size, 100).to(device)\n",
        "        gen_labels = torch.randint(0, 10, (batch_size,)).to(device)\n",
        "        fake_imgs = generator(z, gen_labels)\n",
        "        fake_loss = adversarial_loss(discriminator(fake_imgs.detach(), gen_labels), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # 訓練 Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        z = torch.randn(batch_size, 100).to(device)\n",
        "        gen_labels = torch.randint(0, 10, (batch_size,)).to(device)\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs, gen_labels), torch.ones(batch_size, 1).to(device))\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        g_losses.append(g_loss.item())\n",
        "        d_losses.append(d_loss.item())\n",
        "        pbar.set_postfix({'D_loss': d_loss.item(), 'G_loss': g_loss.item()})\n",
        "\n",
        "    return np.mean(g_losses), np.mean(d_losses)\n",
        "\n",
        "# cGAN 生成函數\n",
        "def generate_all_digits_cgan(generator, samples_per_digit=10, latent_dim=100):\n",
        "    generator.eval()\n",
        "    all_samples = []\n",
        "    with torch.no_grad():\n",
        "        for digit in range(10):\n",
        "            z = torch.randn(samples_per_digit, latent_dim).to(device)\n",
        "            labels = torch.full((samples_per_digit,), digit).to(device)\n",
        "            samples = generator(z, labels)\n",
        "            samples = (samples + 1) / 2\n",
        "            all_samples.append(samples.cpu())\n",
        "    return torch.cat(all_samples, dim=0)\n",
        "\n",
        "def visualize_cgan_grid(images, title='cGAN Generated Digits (0-9)', save_path=None):\n",
        "    fig, axes = plt.subplots(10, 10, figsize=(15, 15))\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            idx = i * 10 + j\n",
        "            axes[i, j].imshow(images[idx].squeeze(), cmap='gray')\n",
        "            axes[i, j].axis('off')\n",
        "            if j == 0:\n",
        "                axes[i, j].set_ylabel(f'{i}', fontsize=12, rotation=0, labelpad=20)\n",
        "    plt.suptitle(title, fontsize=18, y=0.995)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def plot_cgan_training_curve(g_losses, d_losses, save_path=None):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(g_losses, label='Generator Loss', alpha=0.7)\n",
        "    plt.plot(d_losses, label='Discriminator Loss', alpha=0.7)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('cGAN Training Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "or0Ikq8KgMkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 6. cGAN 執行訓練 ====================\n",
        "print(\"=\" * 60)\n",
        "print(\"開始訓練 cGAN (Conditional GAN)\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# cGAN 訓練參數\n",
        "latent_dim_cgan = 100\n",
        "num_classes = 10\n",
        "hidden_dim_cgan = 512\n",
        "G_lr_cgan = 2e-4\n",
        "D_lr_cgan = 5e-5\n",
        "num_epochs_cgan = 50\n",
        "\n",
        "# 建立模型\n",
        "cgan_generator = ConditionalGenerator(latent_dim=latent_dim_cgan, num_classes=num_classes, hidden_dim=hidden_dim_cgan).to(device)\n",
        "cgan_discriminator = ConditionalDiscriminator(num_classes=num_classes, hidden_dim=hidden_dim_cgan).to(device)\n",
        "optimizer_G_cgan = optim.Adam(cgan_generator.parameters(), lr=G_lr_cgan, betas=(0.5, 0.999))\n",
        "optimizer_D_cgan = optim.Adam(cgan_discriminator.parameters(), lr=D_lr_cgan, betas=(0.5, 0.999))\n",
        "\n",
        "print(f'cGAN 配置:')\n",
        "print(f'Latent Dim: {latent_dim_cgan}, Classes: {num_classes}, Hidden Dim: {hidden_dim_cgan}')\n",
        "print(f'Learning Rate: {G_lr_cgan}, Epochs: {num_epochs_cgan}\\n')\n",
        "\n",
        "cgan_g_losses = []\n",
        "cgan_d_losses = []\n",
        "\n",
        "for epoch in range(1, num_epochs_cgan + 1):\n",
        "    g_loss, d_loss = train_cgan(cgan_generator, cgan_discriminator, train_loader, optimizer_G_cgan, optimizer_D_cgan, epoch, device)\n",
        "    cgan_g_losses.append(g_loss)\n",
        "    cgan_d_losses.append(d_loss)\n",
        "    print(f'Epoch {epoch}: G_loss = {g_loss:.4f}, D_loss = {d_loss:.4f}')\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        samples = generate_all_digits_cgan(cgan_generator, samples_per_digit=10, latent_dim=latent_dim_cgan)\n",
        "        visualize_cgan_grid(samples, title=f'cGAN Generated (Epoch {epoch})')\n",
        "\n",
        "# cGAN 最終結果\n",
        "plot_cgan_training_curve(cgan_g_losses, cgan_d_losses, save_path='cgan_training_curve.png')\n",
        "final_cgan_samples = generate_all_digits_cgan(cgan_generator, samples_per_digit=10, latent_dim=latent_dim_cgan)\n",
        "visualize_cgan_grid(final_cgan_samples, title='cGAN Final Generated Digits (0-9)', save_path='cgan_final_results.png')\n",
        "torch.save(cgan_generator.state_dict(), 'cgan_generator.pth')\n",
        "torch.save(cgan_discriminator.state_dict(), 'cgan_discriminator.pth')\n",
        "print(f'\\ncGAN 訓練完成! 最終 G_loss: {cgan_g_losses[-1]:.4f}, D_loss: {cgan_d_losses[-1]:.4f}\\n')"
      ],
      "metadata": {
        "id": "bqSIW-8vgRtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 7. Diffusion Model 函式庫及定義 ====================\n",
        "print(\"=\" * 60)\n",
        "print(\"載入 Diffusion Model 模組...\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "import math\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# EMA 實現\n",
        "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
        "    def __init__(self, model, decay, device=\"cpu\"):\n",
        "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
        "            return decay * avg_model_param + (1 - decay) * model_param\n",
        "        super().__init__(model, device, ema_avg, use_buffers=True)\n",
        "\n",
        "# ShuffleNet V2 組件\n",
        "class ChannelShuffle(nn.Module):\n",
        "    def __init__(self, groups):\n",
        "        super().__init__()\n",
        "        self.groups = groups\n",
        "\n",
        "    def forward(self, x):\n",
        "        n, c, h, w = x.shape\n",
        "        x = x.view(n, self.groups, c // self.groups, h, w)\n",
        "        x = x.transpose(1, 2).contiguous().view(n, -1, h, w)\n",
        "        return x\n",
        "\n",
        "class ConvBnSiLu(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.module = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.SiLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.module(x)\n",
        "\n",
        "class ResidualBottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels//2, in_channels//2, 3, 1, 1, groups=in_channels//2),\n",
        "            nn.BatchNorm2d(in_channels//2),\n",
        "            ConvBnSiLu(in_channels//2, out_channels//2, 1, 1, 0)\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            ConvBnSiLu(in_channels//2, in_channels//2, 1, 1, 0),\n",
        "            nn.Conv2d(in_channels//2, in_channels//2, 3, 1, 1, groups=in_channels//2),\n",
        "            nn.BatchNorm2d(in_channels//2),\n",
        "            ConvBnSiLu(in_channels//2, out_channels//2, 1, 1, 0)\n",
        "        )\n",
        "        self.channel_shuffle = ChannelShuffle(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1, x2 = x.chunk(2, dim=1)\n",
        "        x = torch.cat([self.branch1(x1), self.branch2(x2)], dim=1)\n",
        "        x = self.channel_shuffle(x)\n",
        "        return x\n",
        "\n",
        "class ResidualDownsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, 3, 2, 1, groups=in_channels),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            ConvBnSiLu(in_channels, out_channels//2, 1, 1, 0)\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            ConvBnSiLu(in_channels, out_channels//2, 1, 1, 0),\n",
        "            nn.Conv2d(out_channels//2, out_channels//2, 3, 2, 1, groups=out_channels//2),\n",
        "            nn.BatchNorm2d(out_channels//2),\n",
        "            ConvBnSiLu(out_channels//2, out_channels//2, 1, 1, 0)\n",
        "        )\n",
        "        self.channel_shuffle = ChannelShuffle(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([self.branch1(x), self.branch2(x)], dim=1)\n",
        "        x = self.channel_shuffle(x)\n",
        "        return x\n",
        "\n",
        "class TimeMLP(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        t_emb = self.mlp(t).unsqueeze(-1).unsqueeze(-1)\n",
        "        x = x + t_emb\n",
        "        return self.act(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_embedding_dim):\n",
        "        super().__init__()\n",
        "        self.conv0 = nn.Sequential(\n",
        "            *[ResidualBottleneck(in_channels, in_channels) for i in range(3)],\n",
        "            ResidualBottleneck(in_channels, out_channels//2)\n",
        "        )\n",
        "        self.time_mlp = TimeMLP(time_embedding_dim, out_channels, out_channels//2)\n",
        "        self.conv1 = ResidualDownsample(out_channels//2, out_channels)\n",
        "\n",
        "    def forward(self, x, t=None):\n",
        "        x_shortcut = self.conv0(x)\n",
        "        if t is not None:\n",
        "            x = self.time_mlp(x_shortcut, t)\n",
        "        x = self.conv1(x)\n",
        "        return [x, x_shortcut]\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_embedding_dim):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.conv0 = nn.Sequential(\n",
        "            *[ResidualBottleneck(in_channels, in_channels) for i in range(3)],\n",
        "            ResidualBottleneck(in_channels, in_channels//2)\n",
        "        )\n",
        "        self.time_mlp = TimeMLP(time_embedding_dim, in_channels, in_channels//2)\n",
        "        self.conv1 = ResidualBottleneck(in_channels//2, out_channels//2)\n",
        "\n",
        "    def forward(self, x, x_shortcut, t=None):\n",
        "        x = self.upsample(x)\n",
        "        x = torch.cat([x, x_shortcut], dim=1)\n",
        "        x = self.conv0(x)\n",
        "        if t is not None:\n",
        "            x = self.time_mlp(x, t)\n",
        "        x = self.conv1(x)\n",
        "        return x\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, timesteps, time_embedding_dim, in_channels=1, out_channels=1,\n",
        "                 base_dim=32, dim_mults=[2, 4]):\n",
        "        super().__init__()\n",
        "        assert isinstance(dim_mults, (list, tuple))\n",
        "        assert base_dim % 2 == 0\n",
        "\n",
        "        channels = self._cal_channels(base_dim, dim_mults)\n",
        "\n",
        "        self.init_conv = ConvBnSiLu(in_channels, base_dim, 3, 1, 1)\n",
        "        self.time_embedding = nn.Embedding(timesteps, time_embedding_dim)\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList(\n",
        "            [EncoderBlock(c[0], c[1], time_embedding_dim) for c in channels]\n",
        "        )\n",
        "        self.decoder_blocks = nn.ModuleList(\n",
        "            [DecoderBlock(c[1], c[0], time_embedding_dim) for c in channels[::-1]]\n",
        "        )\n",
        "\n",
        "        self.mid_block = nn.Sequential(\n",
        "            *[ResidualBottleneck(channels[-1][1], channels[-1][1]) for i in range(2)],\n",
        "            ResidualBottleneck(channels[-1][1], channels[-1][1]//2)\n",
        "        )\n",
        "\n",
        "        self.final_conv = nn.Conv2d(channels[0][0]//2, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x, t=None):\n",
        "        x = self.init_conv(x)\n",
        "        if t is not None:\n",
        "            t = self.time_embedding(t)\n",
        "\n",
        "        encoder_shortcuts = []\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x, x_shortcut = encoder_block(x, t)\n",
        "            encoder_shortcuts.append(x_shortcut)\n",
        "\n",
        "        x = self.mid_block(x)\n",
        "        encoder_shortcuts.reverse()\n",
        "\n",
        "        for decoder_block, shortcut in zip(self.decoder_blocks, encoder_shortcuts):\n",
        "            x = decoder_block(x, shortcut, t)\n",
        "\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "    def _cal_channels(self, base_dim, dim_mults):\n",
        "        dims = [base_dim * x for x in dim_mults]\n",
        "        dims.insert(0, base_dim)\n",
        "        channels = []\n",
        "        for i in range(len(dims) - 1):\n",
        "            channels.append((dims[i], dims[i+1]))\n",
        "        return channels\n",
        "\n",
        "# Diffusion Model\n",
        "class MNISTDiffusion(nn.Module):\n",
        "    def __init__(self, image_size, in_channels, time_embedding_dim=256,\n",
        "                 timesteps=1000, base_dim=32, dim_mults=[2, 4]):\n",
        "        super().__init__()\n",
        "        self.timesteps = timesteps\n",
        "        self.in_channels = in_channels\n",
        "        self.image_size = image_size\n",
        "\n",
        "        betas = self._cosine_variance_schedule(timesteps)\n",
        "        alphas = 1. - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, dim=-1)\n",
        "\n",
        "        self.register_buffer(\"betas\", betas)\n",
        "        self.register_buffer(\"alphas\", alphas)\n",
        "        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n",
        "        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n",
        "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1. - alphas_cumprod))\n",
        "\n",
        "        self.model = Unet(timesteps, time_embedding_dim, in_channels, in_channels,\n",
        "                         base_dim, dim_mults)\n",
        "\n",
        "    def forward(self, x, noise):\n",
        "        t = torch.randint(0, self.timesteps, (x.shape[0],)).to(x.device)\n",
        "        x_t = self._forward_diffusion(x, t, noise)\n",
        "        pred_noise = self.model(x_t, t)\n",
        "        return pred_noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sampling(self, n_samples, clipped_reverse_diffusion=True, device=\"cuda\"):\n",
        "        x_t = torch.randn((n_samples, self.in_channels, self.image_size, self.image_size)).to(device)\n",
        "\n",
        "        for i in tqdm(range(self.timesteps - 1, -1, -1), desc=\"Diffusion Sampling\"):\n",
        "            noise = torch.randn_like(x_t).to(device)\n",
        "            t = torch.tensor([i for _ in range(n_samples)]).to(device)\n",
        "\n",
        "            if clipped_reverse_diffusion:\n",
        "                x_t = self._reverse_diffusion_with_clip(x_t, t, noise)\n",
        "            else:\n",
        "                x_t = self._reverse_diffusion(x_t, t, noise)\n",
        "\n",
        "        x_t = (x_t + 1.) / 2.\n",
        "        return x_t\n",
        "\n",
        "    def _cosine_variance_schedule(self, timesteps, epsilon=0.008):\n",
        "        steps = torch.linspace(0, timesteps, steps=timesteps + 1, dtype=torch.float32)\n",
        "        f_t = torch.cos(((steps / timesteps + epsilon) / (1.0 + epsilon)) * math.pi * 0.5) ** 2\n",
        "        betas = torch.clip(1.0 - f_t[1:] / f_t[:timesteps], 0.0, 0.999)\n",
        "        return betas\n",
        "\n",
        "    def _forward_diffusion(self, x_0, t, noise):\n",
        "        assert x_0.shape == noise.shape\n",
        "        return self.sqrt_alphas_cumprod.gather(-1, t).reshape(x_0.shape[0], 1, 1, 1) * x_0 + \\\n",
        "               self.sqrt_one_minus_alphas_cumprod.gather(-1, t).reshape(x_0.shape[0], 1, 1, 1) * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _reverse_diffusion(self, x_t, t, noise):\n",
        "        pred = self.model(x_t, t)\n",
        "        alpha_t = self.alphas.gather(-1, t).reshape(x_t.shape[0], 1, 1, 1)\n",
        "        alpha_t_cumprod = self.alphas_cumprod.gather(-1, t).reshape(x_t.shape[0], 1, 1, 1)\n",
        "        beta_t = self.betas.gather(-1, t).reshape(x_t.shape[0], 1, 1, 1)\n",
        "        sqrt_one_minus_alpha_cumprod_t = self.sqrt_one_minus_alphas_cumprod.gather(-1, t).reshape(x_t.shape[0], 1, 1, 1)\n",
        "\n",
        "        mean = (1. / torch.sqrt(alpha_t)) * (x_t - ((1.0 - alpha_t) / sqrt_one_minus_alpha_cumprod_t) * pred)\n",
        "\n",
        "        if t.min() > 0:\n",
        "            alpha_t_cumprod_prev = self.alphas_cumprod.gather(-1, t - 1).reshape(x_t.shape[0], 1, 1, 1)\n",
        "            std = torch.sqrt(beta_t * (1. - alpha_t_cumprod_prev) / (1. - alpha_t_cumprod))\n",
        "        else:\n",
        "            std = 0.0\n",
        "\n",
        "        return mean + std * noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _reverse_diffusion_with_clip(self, x_t, t, noise):\n",
        "        pred = self.model(x_t, t)\n",
        "        alpha_t = self.alphas.gather(-1, t).reshape(x_t.shape[0], 1, 1, 1)\n",
        "        alpha_t_cumprod = self.alphas_cumprod.gather(-1, t).reshape(x_t.shape[0], 1, 1, 1)\n",
        "        beta_t = self.betas.gather(-1, t).reshape(x_t.shape[0], 1, 1, 1)\n",
        "\n",
        "        x_0_pred = torch.sqrt(1. / alpha_t_cumprod) * x_t - torch.sqrt(1. / alpha_t_cumprod - 1.) * pred\n",
        "        x_0_pred.clamp_(-1., 1.)\n",
        "\n",
        "        if t.min() > 0:\n",
        "            alpha_t_cumprod_prev = self.alphas_cumprod.gather(-1, t - 1).reshape(x_t.shape[0], 1, 1, 1)\n",
        "            mean = (beta_t * torch.sqrt(alpha_t_cumprod_prev) / (1. - alpha_t_cumprod)) * x_0_pred + \\\n",
        "                   ((1. - alpha_t_cumprod_prev) * torch.sqrt(alpha_t) / (1. - alpha_t_cumprod)) * x_t\n",
        "            std = torch.sqrt(beta_t * (1. - alpha_t_cumprod_prev) / (1. - alpha_t_cumprod))\n",
        "        else:\n",
        "            mean = (beta_t / (1. - alpha_t_cumprod)) * x_0_pred\n",
        "            std = 0.0\n",
        "\n",
        "        return mean + std * noise\n",
        "\n",
        "# Diffusion 訓練函數\n",
        "def train_diffusion_epoch(model, model_ema, train_loader, optimizer, scheduler,\n",
        "                         loss_fn, device, ema_update_interval=10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    global_steps = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Diffusion Training')\n",
        "    for image, _ in pbar:\n",
        "        noise = torch.randn_like(image).to(device)\n",
        "        image = image.to(device)\n",
        "\n",
        "        pred = model(image, noise)\n",
        "        loss = loss_fn(pred, noise)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "\n",
        "        if global_steps % ema_update_interval == 0:\n",
        "            model_ema.update_parameters(model)\n",
        "\n",
        "        global_steps += 1\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.6f}'})\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def plot_diffusion_training_curve(losses, save_path='diffusion_training_curve.png'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses, label='MSE Loss', alpha=0.7)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Diffusion Model Training Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "rCBEQPdNgVke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 8. Diffusion Model 執行訓練 ====================\n",
        "print(\"=\" * 60)\n",
        "print(\"開始訓練 Diffusion Model\")\n",
        "print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Diffusion 資料載入（需要歸一化到 [-1, 1]）\n",
        "transform_diffusion = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "train_dataset_diffusion = datasets.MNIST(root='./data', train=True, download=True, transform=transform_diffusion)\n",
        "train_loader_diffusion = DataLoader(train_dataset_diffusion, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# Diffusion 訓練參數\n",
        "num_epochs_diffusion = 200\n",
        "timesteps = 2000\n",
        "base_dim_diffusion = 128\n",
        "lr_diffusion = 0.0008\n",
        "\n",
        "# 建立模型\n",
        "diffusion_model = MNISTDiffusion(\n",
        "    timesteps=timesteps,\n",
        "    image_size=28,\n",
        "    in_channels=1,\n",
        "    base_dim=base_dim_diffusion,\n",
        "    dim_mults=[2, 4]\n",
        ").to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in diffusion_model.parameters())\n",
        "print(f'Diffusion 模型參數量: {total_params:,} ({total_params/1e6:.2f}M)')\n",
        "\n",
        "# EMA 設定\n",
        "model_ema_steps = 10\n",
        "model_ema_decay = 0.995\n",
        "adjust = 1 * batch_size * model_ema_steps / num_epochs_diffusion\n",
        "alpha = 1.0 - model_ema_decay\n",
        "alpha = min(1.0, alpha * adjust)\n",
        "diffusion_model_ema = ExponentialMovingAverage(diffusion_model, device=device, decay=1.0 - alpha)\n",
        "\n",
        "# 優化器和調度器\n",
        "optimizer_diffusion = optim.Adam(diffusion_model.parameters(), lr=lr_diffusion)\n",
        "scheduler_diffusion = OneCycleLR(\n",
        "    optimizer_diffusion, lr_diffusion,\n",
        "    total_steps=num_epochs_diffusion * len(train_loader_diffusion),\n",
        "    pct_start=0.25,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "loss_fn_diffusion = nn.MSELoss(reduction='mean')\n",
        "\n",
        "print(f'Diffusion 配置:')\n",
        "print(f'Timesteps: {timesteps}, Base Dim: {base_dim_diffusion}')\n",
        "print(f'Learning Rate: {lr_diffusion}, Epochs: {num_epochs_diffusion}\\n')\n",
        "\n",
        "diffusion_losses = []\n",
        "\n",
        "for epoch in range(1, num_epochs_diffusion + 1):\n",
        "    loss = train_diffusion_epoch(\n",
        "        diffusion_model, diffusion_model_ema, train_loader_diffusion,\n",
        "        optimizer_diffusion, scheduler_diffusion, loss_fn_diffusion,\n",
        "        device, ema_update_interval=model_ema_steps\n",
        "    )\n",
        "    diffusion_losses.append(loss)\n",
        "    print(f'Epoch {epoch}/{num_epochs_diffusion}: Loss = {loss:.4f}')\n",
        "\n",
        "    if epoch % 20 == 0 or epoch == num_epochs_diffusion:\n",
        "        print(f\"\\n生成 Epoch {epoch} 的樣本...\")\n",
        "        diffusion_model_ema.eval()\n",
        "        samples = diffusion_model_ema.module.sampling(\n",
        "            n_samples=10,\n",
        "            clipped_reverse_diffusion=True,\n",
        "            device=device\n",
        "        )\n",
        "        visualize_generated(samples, title=f'Diffusion Generated (Epoch {epoch})')\n",
        "\n",
        "# Diffusion 最終結果\n",
        "plot_diffusion_training_curve(diffusion_losses, save_path='diffusion_training_curve.png')\n",
        "diffusion_model_ema.eval()\n",
        "final_diffusion_samples = diffusion_model_ema.module.sampling(\n",
        "    n_samples=10,\n",
        "    clipped_reverse_diffusion=True,\n",
        "    device=device\n",
        ")\n",
        "visualize_generated(final_diffusion_samples, title='Diffusion Final Generated Images', save_path='diffusion_final_results.png')\n",
        "torch.save({\n",
        "    'model': diffusion_model.state_dict(),\n",
        "    'model_ema': diffusion_model_ema.state_dict()\n",
        "}, 'mnist_diffusion_final.pt')\n",
        "print(f'\\nDiffusion Model 訓練完成! 最終 Loss: {diffusion_losses[-1]:.4f}\\n')\n"
      ],
      "metadata": {
        "id": "rsfHpXfygfVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qs3_4mv-giJ4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}